Markov Chain Monte Carlo
========================

> B. Walsh

### Monte Carlo

The original **Monte Carlo** approach was a method t oused random number generation to compute integrals. Suppose we wish to compute a complex integral

![\int_a^b h(x)\,dx](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cint_a%5Eb%20h%28x%29%5C%2Cdx)

If we can decompose h(x) into the production of a function f(x) and a probability density function p(x) defined over the interval (a,b), then

![\int_a^b h(x)\,dx=\int_a^b f(x)p(x)\,dx=E_{p(x)}[f(x)]](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cint_a%5Eb%20h%28x%29%5C%2Cdx%3D%5Cint_a%5Eb%20f%28x%29p%28x%29%5C%2Cdx%3DE_%7Bp%28x%29%7D%5Bf%28x%29%5D)

so that the integral can be expressed as an expectation of f(x) over the density p(x). Thus, from the law of large numbers, if we draw a large number x1,...xn of random variables from the density p(x), then

![\int_a^b h(x)\,dx=E_{p(x)}[f(x)]\simeq\frac{1}{n}\sum_{i=1}^{n}f(x_i)](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cint_a%5Eb%20h%28x%29%5C%2Cdx%3DE_%7Bp%28x%29%7D%5Bf%28x%29%5D%5Csimeq%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Df%28x_i%29)

This is referred to as **Monte Carlo integration**.

Monte Carlo integration can be used to approximate posterior (or marginal posterior) distributions required for a Bayesian analysis. Consider the integral

![I(y)=\int f(y|x)p(x)\,dx](http://latex.codecogs.com/gif.latex?%5Cbg_white%20I%28y%29%3D%5Cint%20f%28y%7Cx%29p%28x%29%5C%2Cdx)

which we approximate by

![\hat{I}(y)=\frac{1}{n}\sum_{i=1}^{n}f(y|x_i)](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Chat%7BI%7D%28y%29%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Df%28y%7Cx_i%29).

### Importance Sampling

Suppose the density function p(x) roughly approximates the density (of interest) q(x), then

![\int f(x)q(x)\,dx=\int f(x)\left(\frac{q(x)}{p(x)} \right )p(x)\,dx=E_{p(x)}\left[f(x)\left(\frac{q(x)}{p(x)}\right) \right ]http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cint%20f%28x%29q%28x%29%5C%2Cdx%3D%5Cint%20f%28x%29%5Cleft%28%5Cfrac%7Bq%28x%29%7D%7Bp%28x%29%7D%20%5Cright%20%29p%28x%29%5C%2Cdx%3DE_%7Bp%28x%29%7D%5Cleft%5Bf%28x%29%5Cleft%28%5Cfrac%7Bq%28x%29%7D%7Bp%28x%29%7D%5Cright%29%20%5Cright%5D)

This forms the basis for the method of **importance sampling** with

![\int f(x)q(x)\,dx=E_{p(x)}\left[f(x)\frac{q(x)}{p(x)} \right ]\simeq\frac{1}{n}\sum_{i=1}^{n}f(x_i)\left(\frac{q(x)}{p(x)}\right) ](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cint%20f%28x%29q%28x%29%5C%2Cdx%3DE_%7Bp%28x%29%7D%5Cleft%5Bf%28x%29%5Cfrac%7Bq%28x%29%7D%7Bp%28x%29%7D%20%5Cright%20%5D%5Csimeq%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Df%28x_i%29%5Cleft%28%5Cfrac%7Bq%28x%29%7D%7Bp%28x%29%7D%20%5Cright%20%29)

where the xi are drawn from the distribution given by p(x). For example, if we are interested in a marginal density as a function of y,

![J(y)=\int f(y|x)q(x)\,dx](http://latex.codecogs.com/gif.latex?%5Cbg_white%20J%28y%29%3D%5Cint%20f%28y%7Cx%29q%28x%29%5C%2Cdx)

we approximate this by

![J(y)\simeq\frac{1}{n}\sum_{i=1}^{n}f(y|x_i)\left(\frac{q(x_i)}{p(x_i)}\right)](http://latex.codecogs.com/gif.latex?%5Cbg_white%20J%28y%29%5Csimeq%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Df%28y%7Cx_i%29%5Cleft%28%5Cfrac%7Bq%28x_i%29%7D%7Bp%28x_i%29%7D%5Cright%29)

An alternative formulation of importance sampling is to use

![\int f(x)q(x)\,dx\simeq\hat{I}=\sum_{i=1}^{n}w_if(x_i)\Big/\sum_{i=1}^{n}w_i](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cint%20f%28x%29q%28x%29%5C%2Cdx%5Csimeq%5Chat%7BI%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dw_if%28x_i%29%5CBig/%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dw_i)

This has an associated Monte Carlo variance of

![\mathrm{Var}(\hat{I})=\sum_{i=1}^{n}w_i\left(f(x_i)-\hat{I}\right )\big/\sum_{i=1}^{n}w_i](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cmathrm%7BVar%7D%28%5Chat%7BI%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dw_i%5Cleft%28f%28x_i%29-%5Chat%7BI%7D%5Cright%20%29%5Cbig/%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dw_i)


### Markov chain

A **Markov chain** refers to a sequence of random variables (X0,...Xn) generated by a **Markov process**. The random variable is a Markov process if the 
ransition probabilities between different values in the state space depend only on the random variable's current state, i.e.,

![\mathrm{Pr}(X_{t+1}=s_j|X_0=s_k,\cdots,X_t=s_i)=\mathrm{Pr}(X_{t+1}=s_j|X_t=s_i)](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cmathrm%7BPr%7D%28X_%7Bt&plus;1%7D%3Ds_j%7CX_0%3Ds_k%2C%5Ccdots%2CX_t%3Ds_i%29%3D%5Cmathrm%7BPr%7D%28X_%7Bt&plus;1%7D%3Ds_j%7CX_t%3Ds_i%29)

A particular chain is defined most critically by its transition probabilities process at state space si moves to state sj in a single step,

![P(i,j)=P(i\rightarrow j)=\mathrm{Pr}(X_{t+1}=s_j|X_t=s_i)](http://latex.codecogs.com/gif.latex?%5Cbg_white%20P%28i%2Cj%29%3DP%28i%5Crightarrow%20j%29%3D%5Cmathrm%7BPr%7D%28X_%7Bt&plus;1%7D%3Ds_j%7CX_t%3Ds_i%29)

Let ![\pi_j(t)=\mathrm{Pr}(X_t=s_j)](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cpi_j%28t%29%3D%5Cmathrm%7BPr%7D%28X_t%3Ds_j%29) denote the probability that the chain is in state j at time t and

![\boldsymbol{\pi}(t)=(\pi_i(t),\pi_j(t),\cdots,\pi_k(t))](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cboldsymbol%7B%5Cpi%7D%28t%29%3D%28%5Cpi_i%28t%29%2C%5Cpi_j%28t%29%2C%5Ccdots%2C%5Cpi_k%28t%29%29) denote the row vector of the state space probabilities at step t.

The probability that the chain has state value si at time (or step) t+1 is given by the **Chapman-Kolomogrov equation**, which sums over the probability of being in a particular state at the current step and the transition probability from that state into state si

![\begin{align*}\pi_i(t+1)=&\mathrm{Pr}(X_{t+1}=s_i)\\&=\sum_k\mathrm{Pr}(X_{t+1}=s_i|X_t=s_k)\cdot\mathrm{Pr}(X_t=s_k)\\&=\sum_kP(k\rightarrow i)\pi_k(t)=\sum_kP(k,i)\pi_k(t)\end{align*} ](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cbegin%7Balign*%7D%5Cpi_i%28t&plus;1%29%3D%26%5Cmathrm%7BPr%7D%28X_%7Bt&plus;1%7D%3Ds_i%29%5C%5C%26%3D%5Csum_k%5Cmathrm%7BPr%7D%28X_%7Bt&plus;1%7D%3Ds_i%7CX_t%3Ds_k%29%5Ccdot%5Cmathrm%7BPr%7D%28X_t%3Ds_k%29%5C%5C%26%3D%5Csum_kP%28k%5Crightarrow%20i%29%5Cpi_k%28t%29%3D%5Csum_kP%28k%2Ci%29%5Cpi_k%28t%29%5Cend%7Balign*%7D)

Define the probability transition matrix P as the matrix whose i,jth element is P(i,j), the Chapman-Kolomogrov equation becomes

![\boldsymbol{\pi}(t+1)=\boldsymbol{\pi}(t)\mathbf{P}](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cboldsymbol%7B%5Cpi%7D%28t&plus;1%29%3D%5Cboldsymbol%7B%5Cpi%7D%28t%29%5Cmathbf%7BP%7D)

Using the matrix form, we have

![\boldsymbol{\pi}(t)=\boldsymbol{\pi}(t-1)\mathbf{P}=(\boldsymbol{\pi}(t-2)\mathbf{P})\mathbf{P}=\boldsymbol{\pi}(t-2)\mathbf{P}^2](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cboldsymbol%7B%5Cpi%7D%28t%29%3D%5Cboldsymbol%7B%5Cpi%7D%28t-1%29%5Cmathbf%7BP%7D%3D%28%5Cboldsymbol%7B%5Cpi%7D%28t-2%29%5Cmathbf%7BP%7D%29%5Cmathbf%7BP%7D%3D%5Cboldsymbol%7B%5Cpi%7D%28t-2%29%5Cmathbf%7BP%7D%5E2)

and 

![\boldsymbol{\pi}(t)=\boldsymbol{\pi}(0)\mathbf{P}^t](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cboldsymbol%7B%5Cpi%7D%28t%29%3D%5Cboldsymbol%7B%5Cpi%7D%280%29%5Cmathbf%7BP%7D%5Et)

If transition matrix P is

1. irreducibile
2. communicate
3. aperiodic

then a Markov chain may reach a **stationary distibution π\***, where the vector of probabilitieis of being in any particular gvin state is independent of the initial condition. The stationary distribution statisfies

![\boldsymbol{\pi}^*=\boldsymbol{\pi}^*\mathbf{P}](http://latex.codecogs.com/gif.latex?%5Cbg_white%20%5Cboldsymbol%7B%5Cpi%7D%5E*%3D%5Cboldsymbol%7B%5Cpi%7D%5E*%5Cmathbf%7BP%7D)

In other words, π\* is the left eigenvalue associated with the eigenvalue λ=1 of P. A sufficient condition for a unique stationary distribution is that the **detailed balance** equation holds (for all i and j),

![P(j\rightarrow k)\pi_j^*=P(k\rightarrow j)\pi_k^*](http://latex.codecogs.com/gif.latex?P%28j%5Crightarrow%20k%29%5Cpi_j%5E*%3DP%28k%5Crightarrow%20j%29%5Cpi_k%5E*)

This is also called the reversibility condition

### Discrete to Continuous

The basic idea of discrete-state Markov chain can be generalized to a continuous state markov process by having a probability kernel P(x,y) that statisfies

![\int P(x,y)\,dy=1](http://latex.codecogs.com/gif.latex?%5Cint%20P%28x%2Cy%29%5C%2Cdy%3D1)

and the continuous extension of the Chapman-Kolomogrovequation becomes

![\pi_t(y)=\int_\pi_{t-1}(x)P(x,y)\,dy](http://latex.codecogs.com/gif.latex?%5Cpi_t%28y%29%3D%5Cint_%5Cpi_%7Bt-1%7D%28x%29P%28x%2Cy%29%5C%2Cdy)

At equilibrium, the stationary distribution satisfies

![\pi^*(y)=\int \pi^*(x)P(x,y)\,dy](http://latex.codecogs.com/gif.latex?%5Cpi%5E*%28y%29%3D%5Cint%20%5Cpi%5E*%28x%29P%28x%2Cy%29%5C%2Cdy)
